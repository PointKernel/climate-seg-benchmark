{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "import os\n",
    "import re\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input and output dirs\n",
    "datadirs = [\"../data\"]\n",
    "outputdir = \".\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "selectkeys = ['Kernel Name', 'Metric Name', 'Metric Type']\n",
    "resultkeys = ['Kernel Name', 'Calls']\n",
    "\n",
    "def merge_results(df, name):\n",
    "    if name+'_x' and name+'_y' in df.columns:\n",
    "        df[name] = df[name+'_x'] + df[name+'_y']\n",
    "        del df[name+'_x']\n",
    "        del df[name+'_y']\n",
    "        \n",
    "def transpose_frame(df_results, df_metrics):    \n",
    "    tc_peak_perf_flops = 125*10**12\n",
    "\n",
    "    # Cleanup: group metric values\n",
    "    metricdf = df_metrics.copy()\n",
    "    metricdf = metricdf.groupby(selectkeys).sum().reset_index()\n",
    "    metricdf = metricdf[['Kernel Name', 'Calls', 'Metric Name', 'Metric Type', 'Metric Value']]\n",
    "    metricdf.sort_values(by=resultkeys)\n",
    "    \n",
    "    # Raise if data not consistent\n",
    "    if df_results.empty:\n",
    "        df_results = metricdf[resultkeys].drop_duplicates().reset_index(drop=True).sort_values(by=resultkeys).copy()\n",
    "        print(df_results)\n",
    "    else:\n",
    "        tmpMetricdf = metricdf[resultkeys].drop_duplicates().reset_index(drop=True).sort_values(by=resultkeys).copy()\n",
    "        tmpResultdf = df_results[resultkeys].drop_duplicates().reset_index(drop=True).sort_values(by=resultkeys).copy()\n",
    "        if not tmpMetricdf.equals(tmpResultdf):\n",
    "            print(\"\\n##### Data not in the output DF\")\n",
    "            df = tmpMetricdf.merge(tmpResultdf, how = 'outer' ,indicator=True).loc[lambda x : x['_merge']=='left_only']\n",
    "            print(df)\n",
    "            print(\"##### Data not in the current DF\")\n",
    "            df = tmpResultdf.merge(tmpMetricdf, how = 'outer' ,indicator=True).loc[lambda x : x['_merge']=='left_only']\n",
    "            print(df)\n",
    "            raise ValueError(\"Data not consistent\")\n",
    "\n",
    "    metriclist = metricdf['Metric Name'].unique()\n",
    "    \n",
    "\n",
    "    ####### Get number of FLOPs\n",
    "    \n",
    "    ### FMA FLOPs = number of FMA instructions x 2\n",
    "    metricdf.loc[metricdf[\"Metric Name\"].str.contains(\"fma_pred_on\"), [\"Metric Value\"]] *= 2\n",
    "    \n",
    "    \n",
    "    ### FP32 FLOPs\n",
    "    metrics = ['smsp__sass_thread_inst_executed_op_fadd_pred_on',\n",
    "               'smsp__sass_thread_inst_executed_op_ffma_pred_on',\n",
    "               'smsp__sass_thread_inst_executed_op_fmul_pred_on']\n",
    "    if any(m in metriclist for m in metrics):\n",
    "        tmpdf = metricdf.loc[ metricdf[\"Metric Name\"].isin(metrics), resultkeys+[\"Metric Value\"] ].copy()\n",
    "        metricname = 'FP32 FLOPs'\n",
    "        tmpdf = tmpdf.groupby(resultkeys).sum().reset_index().rename(columns={\"Metric Value\": metricname})\n",
    "        # Merge current df with the result df\n",
    "        df_results = df_results.merge(tmpdf, on=resultkeys, how=\"outer\")\n",
    "        # Sum up if exits and remove duplicates\n",
    "        merge_results(df_results, metricname)\n",
    "        #print(df_results)\n",
    "    \n",
    "    \n",
    "    ### FP16 FLOPs\n",
    "    metrics = ['smsp__sass_thread_inst_executed_op_hadd_pred_on',\n",
    "               'smsp__sass_thread_inst_executed_op_hfma_pred_on',\n",
    "               'smsp__sass_thread_inst_executed_op_hmul_pred_on']\n",
    "    if any(m in metriclist for m in metrics):\n",
    "        tmpdf = metricdf.loc[ metricdf[\"Metric Name\"].isin(metrics), resultkeys+[\"Metric Value\"] ].copy()\n",
    "        metricname = 'FP16 FLOPs'\n",
    "        tmpdf = tmpdf.groupby(resultkeys).sum().reset_index().rename(columns={\"Metric Value\": metricname})\n",
    "        # Merge current df with the result df\n",
    "        df_results = df_results.merge(tmpdf, on=resultkeys, how=\"outer\")\n",
    "        # Sum up if exits and remove duplicates\n",
    "        merge_results(df_results, metricname)\n",
    "        #print(df_results)\n",
    "    \n",
    "    \n",
    "    ### TC FLOP Rates\n",
    "    if any(\"tensor_op_hmma.avg.pct_of_peak\" in m for m in metriclist):\n",
    "        tmpdf = metricdf.loc[ metricdf[\"Metric Name\"].str.contains(\"tensor_op_hmma.avg.pct_of_peak\"), resultkeys+[\"Metric Value\"] ].reset_index().copy()\n",
    "        tmpdf[\"Utilization\"] = 0.01 * tmpdf[\"Metric Value\"] / tmpdf['Calls']\n",
    "        metricname = \"TC FLOP Rates\"\n",
    "        tmpdf[metricname] = tc_peak_perf_flops * tmpdf[\"Utilization\"]\n",
    "        # merge\n",
    "        df_results = df_results.merge(tmpdf[resultkeys+[metricname]], on=resultkeys, how=\"outer\")\n",
    "        merge_results(df_results, metricname)\n",
    "        #print(df_results)\n",
    "    \n",
    "    ### Total FLOPs\n",
    "    #metricdf[\"FLOPs Avg\"] = metricdf[\"FP32 FLOPs Avg\"] + metricdf[\"FP16 FLOPs Avg\"] + metricdf[\"TC FLOPs Avg\"] #+ metricdf[\"FP64 FLOPs\"]\n",
    "\n",
    "    \n",
    "    ### FLOPs fractions\n",
    "    #metricdf[\"FP64 FLOPs Fraction\"] = metricdf[\"FP64 FLOPs\"]/metricdf[\"FLOPs\"]\n",
    "    #metricdf[\"FP32 FLOPs Fraction Avg\"] = metricdf[\"FP32 FLOPs Avg\"]/metricdf[\"FLOPs Avg\"]\n",
    "    #metricdf[\"FP16 FLOPs Fraction Avg\"] = metricdf[\"FP16 FLOPs Avg\"]/metricdf[\"FLOPs Avg\"]\n",
    "    #metricdf[\"TC FLOPs Fraction Avg\"]   = metricdf[\"TC FLOPs Avg\"]/metricdf[\"FLOPs Avg\"]\n",
    "    #print(metricdf)\n",
    "    \n",
    "    \n",
    "\n",
    "    ####### Get timing information\n",
    "\n",
    "    ### CUDA Time\n",
    "    if any(\"smsp__cycles_elapsed\" in m for m in metriclist):\n",
    "        # get cycles\n",
    "        metricname = \"CUDA Cycles\"\n",
    "        cyclesdf = metricdf.loc[(metricdf[\"Metric Name\"]==\"smsp__cycles_elapsed\") & (metricdf[\"Metric Type\"]==\"total\"),\n",
    "                               resultkeys+[\"Metric Value\"]].reset_index(drop=True).rename(columns={\"Metric Value\": metricname}).copy()\n",
    "        df_results = df_results.merge(cyclesdf, on=resultkeys, how=\"outer\")\n",
    "        # get rates\n",
    "        metricname = \"CUDA Rates\"\n",
    "        ratesdf = metricdf.loc[(metricdf[\"Metric Name\"]==\"smsp__cycles_elapsed\") & (metricdf[\"Metric Type\"]==\"rate\"),\n",
    "                               resultkeys+[\"Metric Value\"]].reset_index(drop=True).rename(columns={\"Metric Value\": metricname}).copy()        \n",
    "        df_results = df_results.merge(ratesdf[resultkeys+[metricname]], on=resultkeys, how=\"outer\")\n",
    "        #print(df_results)\n",
    "    \n",
    "    \n",
    "    ### Tensor Core Time\n",
    "    if any(\"smsp__pipe_tensor_op_hmma_cycles_active\" in m for m in metriclist):\n",
    "        # get cycles\n",
    "        metricname = \"TC Cycles\"\n",
    "        cyclesdf = metricdf.loc[(metricdf[\"Metric Name\"]==\"smsp__pipe_tensor_op_hmma_cycles_active\") & (metricdf[\"Metric Type\"]==\"total\"),\n",
    "                               resultkeys+[\"Metric Value\"]].reset_index(drop=True).rename(columns={\"Metric Value\": metricname}).copy()\n",
    "        df_results = df_results.merge(cyclesdf, on=resultkeys, how=\"outer\")\n",
    "        # get rates\n",
    "        metricname = \"TC Rates\"\n",
    "        ratesdf = metricdf.loc[(metricdf[\"Metric Name\"]==\"smsp__pipe_tensor_op_hmma_cycles_active\") & (metricdf[\"Metric Type\"]==\"rate\"),\n",
    "                               resultkeys+[\"Metric Value\"]].reset_index(drop=True).rename(columns={\"Metric Value\": metricname}).copy()        \n",
    "        df_results = df_results.merge(ratesdf[resultkeys+[metricname]], on=resultkeys, how=\"outer\")\n",
    "        #print(df_results)\n",
    "        \n",
    "\n",
    "\n",
    "    ####### Get number of bytes\n",
    "\n",
    "    ### FP32 FLOPs\n",
    "    metrics = ['smsp__sass_thread_inst_executed_op_fadd_pred_on',\n",
    "               'smsp__sass_thread_inst_executed_op_ffma_pred_on',\n",
    "               'smsp__sass_thread_inst_executed_op_fmul_pred_on']\n",
    "    if any(m in metriclist for m in metrics):\n",
    "        tmpdf = metricdf.loc[ metricdf[\"Metric Name\"].isin(metrics), resultkeys+[\"Metric Value\"] ].copy()\n",
    "        metricname = 'FP32 FLOPs'\n",
    "        tmpdf = tmpdf.groupby(resultkeys).sum().reset_index().rename(columns={\"Metric Value\": metricname})\n",
    "        # Merge current df with the result df\n",
    "        df_results = df_results.merge(tmpdf, on=resultkeys, how=\"outer\")\n",
    "        # Sum up if exits and remove duplicates\n",
    "        merge_results(df_results, metricname)\n",
    "        #print(df_results)\n",
    "    \n",
    "    \n",
    "    ### Shared transactions\n",
    "    #project out\n",
    "    if any(\"l1tex__data_pipe_lsu_wavefronts_mem_shared_op\" in m for m in metriclist):\n",
    "        shareddf = metricdf.loc[metricdf[\"Metric Name\"].str.contains(\"l1tex__data_pipe_lsu_wavefronts_mem_shared_op\"), resultkeys+[\"Metric Value\"] ].copy()\n",
    "        metricname = 'Shared Transactions'\n",
    "        shareddf = shareddf.groupby(resultkeys).sum().reset_index().rename(columns={\"Metric Value\": metricname})\n",
    "        df_results = df_results.merge(shareddf, on=resultkeys, how=\"outer\")\n",
    "        merge_results(df_results, metricname)\n",
    "\n",
    "    ### L1 atomic transactions\n",
    "    # project out\n",
    "    metrics = ['l1tex__t_set_accesses_pipe_lsu_mem_global_op_atom',\n",
    "               'l1tex__t_set_accesses_pipe_lsu_mem_global_op_red',\n",
    "               'l1tex__t_set_accesses_pipe_tex_mem_surface_op_atom',\n",
    "               'l1tex__t_set_accesses_pipe_tex_mem_surface_op_red']\n",
    "    if any(m in metriclist for m in metrics):\n",
    "        tmpdf = metricdf.loc[ metricdf[\"Metric Name\"].isin(metrics), resultkeys+[\"Metric Value\"] ].copy()\n",
    "        metricname = 'L1 Atomic Transactions'\n",
    "        tmpdf = tmpdf.groupby(resultkeys).sum().reset_index().rename(columns={\"Metric Value\": metricname})\n",
    "        # Merge current df with the result df\n",
    "        df_results = df_results.merge(tmpdf, on=resultkeys, how=\"outer\")\n",
    "        # Sum up if exits and remove duplicates\n",
    "        merge_results(df_results, metricname)\n",
    "        #print(df_results)\n",
    "        \n",
    "    ### Local transactions\n",
    "    #project out\n",
    "    if any(\"l1tex__t_sectors_pipe_lsu_mem_local_op\" in m for m in metriclist):\n",
    "        localdf = metricdf.loc[metricdf[\"Metric Name\"].str.contains(\"l1tex__t_sectors_pipe_lsu_mem_local_op\"), resultkeys+[\"Metric Value\"] ].copy()\n",
    "        metricname = 'Local Transactions'\n",
    "        localdf = localdf.groupby(resultkeys).sum().reset_index().rename(columns={\"Metric Value\": metricname})\n",
    "        df_results = df_results.merge(localdf, on=resultkeys, how=\"outer\")\n",
    "        merge_results(df_results, metricname)\n",
    "        \n",
    "    return df_results\n",
    "    \n",
    "    ### Global transactions \n",
    "    # project out\n",
    "    globaldf = metricdf.loc[metricdf[\"Metric Name\"].str.contains(\"l1tex__t_sectors_pipe_lsu_mem_global_op\"), selectkeys+[\"Metric Value\"] ].copy()\n",
    "    globaldf = globaldf.groupby(selectkeys).sum().reset_index().rename(columns={\"Metric Value\": \"Global Transactions Avg\"})\n",
    "    # add to timings\n",
    "    metricdf = metricdf.merge(globaldf[selectkeys+[\"Global Transactions Avg\"]], on=selectkeys, how=\"inner\")\n",
    "    \n",
    "    \n",
    "    ### L1 Bytes\n",
    "    metricdf[\"L1 Transactions Avg\"] = (metricdf[\"Shared Transactions Avg\"] + metricdf[\"L1 Atomic Transactions Avg\"]\n",
    "                            + metricdf[\"Local Transactions Avg\"] + metricdf[\"Global Transactions Avg\"])\n",
    "    metricdf[\"L1 Bytes Avg\"] = metricdf[\"L1 Transactions Avg\"] * 32\n",
    "    \n",
    "    # clean up\n",
    "    #del metricdf[\"Shared Transactions Avg\"]\n",
    "    #del metricdf[\"L1 Atomic Transactions Avg\"]\n",
    "    #del metricdf[\"Local Transactions Avg\"]\n",
    "    #del metricdf[\"Global Transactions Avg\"]\n",
    "    \n",
    "    \n",
    "    ### L2 atomic & reduction\n",
    "    metricdf.loc[(metricdf[\"Metric Name\"].str.contains(\"lts__t_sectors_op\")) & (metricdf[\"Metric Type\"]==\"total\"), [\"Metric Value\"]] *= 2\n",
    "    \n",
    "    \n",
    "    ### L2 transactions\n",
    "    # project out\n",
    "    l2df = metricdf.loc[metricdf[\"Metric Name\"].str.contains(\"lts__t_sectors_op\"), selectkeys+[\"Metric Value\"] ].copy()\n",
    "    l2df = l2df.groupby(selectkeys).sum().reset_index().rename(columns={\"Metric Value\": \"L2 Transactions Avg\"})\n",
    "    l2df[\"L2 Bytes Avg\"] = l2df[\"L2 Transactions Avg\"] * 32\n",
    "    # add to timings\n",
    "    metricdf = metricdf.merge(l2df[selectkeys+[\"L2 Transactions Avg\", \"L2 Bytes Avg\"]], on=selectkeys, how=\"inner\")\n",
    "    \n",
    "    \n",
    "    ### DRAM Bytes\n",
    "    # project out\n",
    "    dramdf = metricdf[ metricdf[\"Metric Name\"].str.contains(\"dram__sectors\") ].sort_values(selectkeys)\n",
    "    # get reads and writes\n",
    "    dramreadsdf = dramdf.loc[(dramdf[\"Metric Name\"]==\"dram__sectors\") & (dramdf[\"Metric Type\"]==\"read\"), selectkeys+[\"Metric Value\"]]\n",
    "    dramwritesdf = dramdf.loc[(dramdf[\"Metric Name\"]==\"dram__sectors\") & (dramdf[\"Metric Type\"]==\"write\"), selectkeys+[\"Metric Value\"]]\n",
    "    # combine\n",
    "    dramdf = dramwritesdf.merge(dramreadsdf, on=selectkeys, how=\"outer\").fillna(0.)\n",
    "    dramdf[\"DRAM Transactions Avg\"] = dramdf[\"Metric Value_x\"] + dramdf[\"Metric Value_y\"]\n",
    "    dramdf[\"DRAM Bytes Avg\"] = dramdf[\"DRAM Transactions Avg\"] * 32\n",
    "    #print(dramdf[['Name', 'Metric Value_x', 'Metric Value_y']])\n",
    "    metricdf = metricdf.merge(dramdf[selectkeys+[\"DRAM Transactions Avg\", \"DRAM Bytes Avg\"]], on=selectkeys, how=\"inner\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    ####### Clean up and return:\n",
    "    del metricdf[\"Metric Value\"]\n",
    "    del metricdf[\"Metric Name\"]\n",
    "    del metricdf[\"Metric Type\"]\n",
    "    #del metricdf[\"Invocations\"]\n",
    "    metricdf.drop_duplicates(keep = 'first', inplace = True)\n",
    "    \n",
    "\n",
    "    ### Get performance\n",
    "    metricdf[\"Performance GFlop/s\"]      = metricdf[\"FLOPs Avg\"]      / (metricdf[\"CUDA Time Avg\"]*10**9)\n",
    "    metricdf[\"FP32 Performance GFlop/s\"] = metricdf[\"FP32 FLOPs Avg\"] / (metricdf[\"CUDA Time Avg\"]*10**9)\n",
    "    metricdf[\"FP16 Performance GFlop/s\"] = metricdf[\"FP16 FLOPs Avg\"] / (metricdf[\"CUDA Time Avg\"]*10**9)\n",
    "    metricdf[\"TC Performance GFlop/s\"]   = metricdf[\"TC FLOPs Avg\"]   / (metricdf[\"TC Time Avg\"]*10**9)\n",
    "\n",
    "    \n",
    "    ### Get AI\n",
    "    # L1\n",
    "    metricdf[\"L1 AI\"]        = metricdf[\"FLOPs Avg\"]      / metricdf[\"L1 Bytes Avg\"]\n",
    "    metricdf[\"FP32 L1 AI\"]   = metricdf[\"FP32 FLOPs Avg\"] / metricdf[\"L1 Bytes Avg\"]\n",
    "    metricdf[\"FP16 L1 AI\"]   = metricdf[\"FP16 FLOPs Avg\"] / metricdf[\"L1 Bytes Avg\"]\n",
    "    metricdf[\"TC L1 AI\"]     = metricdf[\"TC FLOPs Avg\"]   / metricdf[\"L1 Bytes Avg\"]\n",
    "    # L2\n",
    "    metricdf[\"L2 AI\"]        = metricdf[\"FLOPs Avg\"]      / metricdf[\"L2 Bytes Avg\"]\n",
    "    metricdf[\"FP32 L2 AI\"]   = metricdf[\"FP32 FLOPs Avg\"] / metricdf[\"L2 Bytes Avg\"]\n",
    "    metricdf[\"FP16 L2 AI\"]   = metricdf[\"FP16 FLOPs Avg\"] / metricdf[\"L2 Bytes Avg\"]\n",
    "    metricdf[\"TC L2 AI\"]     = metricdf[\"TC FLOPs Avg\"]   / metricdf[\"L2 Bytes Avg\"]\n",
    "    # DRAM\n",
    "    metricdf[\"DRAM AI\"]      = metricdf[\"FLOPs Avg\"]      / metricdf[\"DRAM Bytes Avg\"]\n",
    "    metricdf[\"FP32 DRAM AI\"] = metricdf[\"FP32 FLOPs Avg\"] / metricdf[\"DRAM Bytes Avg\"]\n",
    "    metricdf[\"FP16 DRAM AI\"] = metricdf[\"FP16 FLOPs Avg\"] / metricdf[\"DRAM Bytes Avg\"]\n",
    "    metricdf[\"TC DRAM AI\"]   = metricdf[\"TC FLOPs Avg\"]   / metricdf[\"DRAM Bytes Avg\"]\n",
    "\n",
    "\n",
    "    ### Cleanup\n",
    "    metricdf.sort_values(by=selectkeys).reset_index(drop=True, inplace=True)\n",
    "    #print(metricdf[['CUDA Time Avg', 'TC Time Avg']])\n",
    "    \n",
    "    return metricdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prefix</th>\n",
       "      <th>file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>cuda_time</td>\n",
       "      <td>../data/cuda_time.log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>ffma_flops</td>\n",
       "      <td>../data/ffma_flops.log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>l1_atom</td>\n",
       "      <td>../data/l1_atom.log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>l1_red</td>\n",
       "      <td>../data/l1_red.log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>shared</td>\n",
       "      <td>../data/shared.log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>tc_flops</td>\n",
       "      <td>../data/tc_flops.log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>tc_time</td>\n",
       "      <td>../data/tc_time.log</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       prefix                    file\n",
       "2   cuda_time   ../data/cuda_time.log\n",
       "0  ffma_flops  ../data/ffma_flops.log\n",
       "4     l1_atom     ../data/l1_atom.log\n",
       "3      l1_red      ../data/l1_red.log\n",
       "6      shared      ../data/shared.log\n",
       "1    tc_flops    ../data/tc_flops.log\n",
       "5     tc_time     ../data/tc_time.log"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#get all the files\n",
    "files = []\n",
    "for datadir in datadirs:\n",
    "    files += [ os.path.join(datadir,x) for x in os.listdir(datadir) if ((os.path.splitext(x)[-1] == \".log\"))]\n",
    "\n",
    "#recs\n",
    "records = []\n",
    "\n",
    "#build feature list:\n",
    "for path in files:\n",
    "    \n",
    "    #filename\n",
    "    file = os.path.basename(path)\n",
    "    \n",
    "    #path\n",
    "    path = os.path.dirname(path)\n",
    "    \n",
    "    #splitup\n",
    "    splt = file.split(\".\")\n",
    "    \n",
    "    prefix = \".\".join(splt[0:-1])\n",
    "    \n",
    "    #append to records\n",
    "    records.append({\"prefix\": prefix, \"file\": os.path.join(path, file)})\n",
    "\n",
    "#put in df\n",
    "recorddf = pd.DataFrame(records).sort_values([\"prefix\"])\n",
    "#with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "display(recorddf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          Kernel Name  Calls\n",
      "0   Volta_hmma_implicit_gemm_fprop_fp32_nhwc_128x1...      2\n",
      "1   Volta_hmma_implicit_gemm_fprop_fp32_nhwc_128x6...      2\n",
      "2   Volta_hmma_implicit_gemm_fprop_fp32_nhwc_256x1...      4\n",
      "3   Volta_hmma_implicit_gemm_wgrad_fp32_nhwc_128x1...      4\n",
      "4   Volta_hmma_implicit_gemm_wgrad_fp32_nhwc_128x6...      1\n",
      "..                                                ...    ...\n",
      "91           volta_fp16_sgemm_fp16_32x32_sliced1x4_nt      1\n",
      "92  volta_s884cudnn_fp16_128x128_ldg8_wgrad_exp_in...      1\n",
      "93  volta_s884cudnn_fp16_128x128_ldg8_wgrad_idx_ex...      7\n",
      "94  volta_s884cudnn_fp16_256x128_ldg8_wgrad_idx_ex...      5\n",
      "95  volta_s884cudnn_fp16_64x64_sliced1x4_ldg8_wgra...      7\n",
      "\n",
      "[96 rows x 2 columns]\n",
      "                                          Kernel Name  Calls   CUDA Cycles  \\\n",
      "0   Volta_hmma_implicit_gemm_fprop_fp32_nhwc_128x1...      2  7.619451e+08   \n",
      "1   Volta_hmma_implicit_gemm_fprop_fp32_nhwc_128x6...      2  8.214659e+07   \n",
      "2   Volta_hmma_implicit_gemm_fprop_fp32_nhwc_256x1...      4  1.461480e+08   \n",
      "3   Volta_hmma_implicit_gemm_wgrad_fp32_nhwc_128x1...      4  1.804578e+08   \n",
      "4   Volta_hmma_implicit_gemm_wgrad_fp32_nhwc_128x6...      1  1.029495e+09   \n",
      "..                                                ...    ...           ...   \n",
      "91           volta_fp16_sgemm_fp16_32x32_sliced1x4_nt      1  7.027400e+08   \n",
      "92  volta_s884cudnn_fp16_128x128_ldg8_wgrad_exp_in...      1  7.401358e+08   \n",
      "93  volta_s884cudnn_fp16_128x128_ldg8_wgrad_idx_ex...      7  1.361048e+09   \n",
      "94  volta_s884cudnn_fp16_256x128_ldg8_wgrad_idx_ex...      5  1.628665e+10   \n",
      "95  volta_s884cudnn_fp16_64x64_sliced1x4_ldg8_wgra...      7  4.923905e+09   \n",
      "\n",
      "      CUDA Rates   FP32 FLOPs  L1 Atomic Transactions  Shared Transactions  \\\n",
      "0   8.363713e+11            0                       0             94010988   \n",
      "1   8.306355e+11            0                       0              6712990   \n",
      "2   1.667595e+12            0                       0              9782171   \n",
      "3   1.663145e+12            0                     540             10690909   \n",
      "4   4.191045e+11            0                     279             87788488   \n",
      "..           ...          ...                     ...                  ...   \n",
      "91  4.197608e+11  28992045056                       0             56761763   \n",
      "92  4.191068e+11    377487360                       0             93227925   \n",
      "93  2.925763e+12    566231040                       0            156792337   \n",
      "94  2.096957e+12    264241152                       0           2007529644   \n",
      "95  2.931906e+12   3712614400                       0            528025130   \n",
      "\n",
      "    TC FLOP Rates     TC Cycles      TC Rates  \n",
      "0    9.594375e+13  5.733089e+08  6.279286e+11  \n",
      "1    5.355625e+13  3.185050e+07  3.220133e+11  \n",
      "2    8.161563e+13  6.370099e+07  7.198852e+11  \n",
      "3    5.822188e+13  6.370099e+07  5.886100e+11  \n",
      "4    6.655000e+13  5.096079e+08  2.086477e+11  \n",
      "..            ...           ...           ...  \n",
      "91   0.000000e+00  0.000000e+00  0.000000e+00  \n",
      "92   9.648750e+13  5.662310e+08  3.179701e+11  \n",
      "93   8.827321e+13  9.555149e+08  1.898086e+12  \n",
      "94   1.126075e+14  1.350461e+10  1.686956e+12  \n",
      "95   6.411071e+13  2.448949e+09  1.416123e+12  \n",
      "\n",
      "[96 rows x 10 columns]\n"
     ]
    }
   ],
   "source": [
    "resdf = pd.DataFrame(columns=resultkeys)\n",
    "\n",
    "#metrics\n",
    "for fin in recorddf[\"file\"]:\n",
    "#for fin in ['../data/time.log']:\n",
    "    #project frame\n",
    "    metricdf = pd.read_csv(fin)\n",
    "    \n",
    "    #fuse read/write metrics together:\n",
    "    unique_metrics = metricdf[\"Metric Name\"].unique()\n",
    "    unique_metrics = set([x.replace(\".sum\",\"\").replace(\"_write\",\"\").replace(\"_read\",\"\").replace(\"_ld\",\"\").replace(\"_st\",\"\") for x in unique_metrics])\n",
    "    #add the metric type\n",
    "    metricdf[\"Metric Type\"] = \"total\"\n",
    "    metricdf[\"Calls\"] = 1\n",
    "    #read\n",
    "    metricdf.loc[ metricdf[ \"Metric Name\" ].str.contains(\"_read\"), \"Metric Type\" ] = \"read\"\n",
    "    metricdf.loc[ metricdf[ \"Metric Name\" ].str.contains(\"_ld\"), \"Metric Type\" ] = \"read\"\n",
    "    #write\n",
    "    metricdf.loc[ metricdf[ \"Metric Name\" ].str.contains(\"_write\"), \"Metric Type\" ] = \"write\"\n",
    "    metricdf.loc[ metricdf[ \"Metric Name\" ].str.contains(\"_st\"), \"Metric Type\" ] = \"write\"\n",
    "    #rate\n",
    "    metricdf.loc[ metricdf[ \"Metric Name\" ].str.contains(\".per_second\"), \"Metric Type\" ] = \"rate\"\n",
    "\n",
    "    for metric in unique_metrics:\n",
    "        metricdf.loc[ metricdf[ \"Metric Name\"].str.startswith(metric), \"Metric Name\" ] = metric\n",
    "    #cleanups\n",
    "    tmpdf = metricdf[[\"Kernel Name\", \"Calls\", \"Metric Name\", \"Metric Type\", \"Metric Value\"]]\n",
    "    #print(tmpdf)\n",
    "\n",
    "    #compute the profile\n",
    "    resdf = transpose_frame(resdf,tmpdf)\n",
    "    #df_profiles.append(profiledf)\n",
    "\n",
    "print(resdf)\n",
    "#profiledf = pd.concat(df_profiles)\n",
    "#profiledf.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "#display(profiledf.columns)\n",
    "#tmplist = ['Name', 'Invocations', 'Pass', 'L1 Transactions Avg', 'L2 Transactions Avg', 'DRAM Transactions Avg']\n",
    "#display(profiledf[tmplist])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute AI Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "#profiledf[ (profiledf[\"Network Name\"]==\"ResNet50-2\") &\\\n",
    "#           (profiledf[\"Input Shape\"]==\"112x112x64\") &\\\n",
    "#           (profiledf[\"Batch Size\"]==16) &\\\n",
    "#           (profiledf[\"Precision\"]==\"FP32\") &\\\n",
    "#           (profiledf[\"Stride Size\"]==2) &\\\n",
    "#           (profiledf[\"Pass\"]==\"forward\") &\\\n",
    "#           (profiledf[\"Kernel Shape\"]==\"7x7x64x64\")\n",
    "#         ]\n",
    "#profiledf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'profiledf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-127-3f7b7847590b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#copy profiledf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mcombineddf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprofiledf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m#get the aggregated performance, including all kernels:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'profiledf' is not defined"
     ]
    }
   ],
   "source": [
    "#sum over all kernels\n",
    "combinedselectkeys = [\"Precision\", \"Network Name\", \"Data Format\", \"Input Shape\", \"Kernel Shape\", \"Stride Size\", \\\n",
    "                     \"Batch Size\", \"Pass\"]\n",
    "\n",
    "#copy profiledf\n",
    "combineddf = profiledf.copy()\n",
    "\n",
    "#get the aggregated performance, including all kernels:\n",
    "#compute weights: multiply all measures by the number of invocations\n",
    "weighted = True\n",
    "if weighted:\n",
    "    #first, get all the names of metrics which need to be weighted\n",
    "    metrics = [x for x in combineddf.columns if \"Avg\" in x]\n",
    "    for metric in metrics:\n",
    "        combineddf[metric] *= combineddf[\"Invocations\"]\n",
    "\n",
    "#sum up\n",
    "combineddf = combineddf.groupby(by=combinedselectkeys).sum()#.reset_index()\n",
    "\n",
    "\n",
    "#the flop fractions need to be recomputed\n",
    "combineddf[\"FP32 FLOPs Fraction Avg\"] = combineddf[\"FP32 FLOPs Avg\"] / combineddf[\"FLOPs Avg\"]\n",
    "combineddf[\"FP16 FLOPs Fraction Avg\"] = combineddf[\"FP16 FLOPs Avg\"] / combineddf[\"FLOPs Avg\"]\n",
    "combineddf[\"TC FLOPs Fraction Avg\"]   = combineddf[\"TC FLOPs Avg\"]   / combineddf[\"FLOPs Avg\"]\n",
    "\n",
    "### Get performance\n",
    "combineddf[\"Performance GFlop/s\"]      = combineddf[\"FLOPs Avg\"]      / (combineddf[\"CUDA Time Avg\"]*10**9)\n",
    "combineddf[\"FP32 Performance GFlop/s\"] = combineddf[\"FP32 FLOPs Avg\"] / (combineddf[\"CUDA Time Avg\"]*10**9)\n",
    "combineddf[\"FP16 Performance GFlop/s\"] = combineddf[\"FP16 FLOPs Avg\"] / (combineddf[\"CUDA Time Avg\"]*10**9)\n",
    "combineddf[\"TC Performance GFlop/s\"]   = combineddf[\"TC FLOPs Avg\"]   / (combineddf[\"TC Time Avg\"]*10**9)\n",
    "\n",
    "\n",
    "### Get AI\n",
    "# L1\n",
    "combineddf[\"L1 AI\"]        = combineddf[\"FLOPs Avg\"]      / combineddf[\"L1 Bytes Avg\"]\n",
    "combineddf[\"FP32 L1 AI\"]   = combineddf[\"FP32 FLOPs Avg\"] / combineddf[\"L1 Bytes Avg\"]\n",
    "combineddf[\"FP16 L1 AI\"]   = combineddf[\"FP16 FLOPs Avg\"] / combineddf[\"L1 Bytes Avg\"]\n",
    "combineddf[\"TC L1 AI\"]     = combineddf[\"TC FLOPs Avg\"]   / combineddf[\"L1 Bytes Avg\"]\n",
    "# L2\n",
    "combineddf[\"L2 AI\"]        = combineddf[\"FLOPs Avg\"]      / combineddf[\"L2 Bytes Avg\"]\n",
    "combineddf[\"FP32 L2 AI\"]   = combineddf[\"FP32 FLOPs Avg\"] / combineddf[\"L2 Bytes Avg\"]\n",
    "combineddf[\"FP16 L2 AI\"]   = combineddf[\"FP16 FLOPs Avg\"] / combineddf[\"L2 Bytes Avg\"]\n",
    "combineddf[\"TC L2 AI\"]     = combineddf[\"TC FLOPs Avg\"]   / combineddf[\"L2 Bytes Avg\"]\n",
    "# DRAM\n",
    "combineddf[\"DRAM AI\"]      = combineddf[\"FLOPs Avg\"]      / combineddf[\"DRAM Bytes Avg\"]\n",
    "combineddf[\"FP32 DRAM AI\"] = combineddf[\"FP32 FLOPs Avg\"] / combineddf[\"DRAM Bytes Avg\"]\n",
    "combineddf[\"FP16 DRAM AI\"] = combineddf[\"FP16 FLOPs Avg\"] / combineddf[\"DRAM Bytes Avg\"]\n",
    "combineddf[\"TC DRAM AI\"]   = combineddf[\"TC FLOPs Avg\"]   / combineddf[\"DRAM Bytes Avg\"]\n",
    "\n",
    "combineddf.sort_values(by=combinedselectkeys).reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#display(combineddf.columns)\n",
    "display(combineddf[['CUDA Time Avg', 'FP32 FLOPs Avg', 'FP16 FLOPs Avg', 'TC FLOPs Avg', 'FLOPs Avg', 'L1 Transactions Avg', 'L2 Transactions Avg', 'DRAM Transactions Avg']])\n",
    "# combineddf.keys\n",
    "# combineddf.columns\n",
    "\n",
    "# combineddf['Name']\n",
    "# combineddf.iloc[0,1]\n",
    "# combineddf.iloc[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combineddf = combineddf.reset_index()\n",
    "#seldf = combineddf[ (combineddf[\"Network Name\"]==\"ResNet50-2\") &\\\n",
    "#           (combineddf[\"Input Shape\"]==\"112x112x64\") &\\\n",
    "#           (combineddf[\"Precision\"]==\"FP32\")]\n",
    "#seldf\n",
    "#combineddf[[\"FP32 L2 AI\", \"FP32 L1 AI\"]]\n",
    "combineddf[[\"L2 AI\", \"L1 AI\", \"DRAM AI\", \"FP32 DRAM AI\", \"FP16 DRAM AI\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profiledf.to_csv(os.path.join(outputdir,\"full_profile.csv\"))\n",
    "combineddf.to_csv(os.path.join(outputdir,\"combined_profile.csv\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
